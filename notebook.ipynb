{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "# e.g. file/directory paths\n",
    "\n",
    "dataset_path = \"Dataset\\DataTensor\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split(dataset, val_portion=0.05, test_portion=0.05):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess\n",
    "\n",
    "Convert MIDI files to .pickle. These will be used as input to our models. Each composor saved by themselves and as a combined set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Dataset/DataTensor\n",
      " [./Dataset/ClassicalMusicMIDI/mozart\\mz_311_1.mid] [./Dataset/ClassicalMusicMIDI/mozart\\mz_311_2.mid] [./Dataset/ClassicalMusicMIDI/mozart\\mz_311_3.mid] [./Dataset/ClassicalMusicMIDI/mozart\\mz_330_1.mid] [./Dataset/ClassicalMusicMIDI/mozart\\mz_330_2.mid] [./Dataset/ClassicalMusicMIDI/mozart\\mz_330_3.mid] [./Dataset/ClassicalMusicMIDI/mozart\\mz_331_1.mid] [./Dataset/ClassicalMusicMIDI/mozart\\mz_331_2.mid] [./Dataset/ClassicalMusicMIDI/mozart\\mz_331_3.mid] [./Dataset/ClassicalMusicMIDI/mozart\\mz_332_1.mid] [./Dataset/ClassicalMusicMIDI/mozart\\mz_332_2.mid] [./Dataset/ClassicalMusicMIDI/mozart\\mz_332_3.mid] [./Dataset/ClassicalMusicMIDI/mozart\\mz_333_1.mid] [./Dataset/ClassicalMusicMIDI/mozart\\mz_333_2.mid] [./Dataset/ClassicalMusicMIDI/mozart\\mz_333_3.mid] [./Dataset/ClassicalMusicMIDI/mozart\\mz_545_1.mid] [./Dataset/ClassicalMusicMIDI/mozart\\mz_545_2.mid] [./Dataset/ClassicalMusicMIDI/mozart\\mz_545_3.mid] [./Dataset/ClassicalMusicMIDI/mozart\\mz_570_1.mid] [./Dataset/ClassicalMusicMIDI/mozart\\mz_570_2.mid] [./Dataset/ClassicalMusicMIDI/mozart\\mz_570_3.mid]"
     ]
    }
   ],
   "source": [
    "%run MusicTransformer/preprocess ./Dataset/ClassicalMusicMIDI/mozart ./Dataset/TrainingMIDI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "// Todo - Detail\n",
    "\n",
    "We now train our 4 networks architectures on our datasets. Changes are made directly to YAML files since they are \"hard\" to edit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Dataset/TrainingMIDI/mozart\n",
      "<class Data has \"21\" files>\n",
      "MusicTransformer(\n",
      "  (Decoder): Encoder(\n",
      "    (embedding): Embedding(391, 256)\n",
      "    (pos_encoding): DynamicPositionEmbedding()\n",
      "    (enc_layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (rga): RelativeGlobalAttention(\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (FFN_pre): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (FFN_suf): Linear(in_features=128, out_features=256, bias=True)\n",
      "        (layernorm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "        (layernorm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): EncoderLayer(\n",
      "        (rga): RelativeGlobalAttention(\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (FFN_pre): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (FFN_suf): Linear(in_features=128, out_features=256, bias=True)\n",
      "        (layernorm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "        (layernorm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): EncoderLayer(\n",
      "        (rga): RelativeGlobalAttention(\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (FFN_pre): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (FFN_suf): Linear(in_features=128, out_features=256, bias=True)\n",
      "        (layernorm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "        (layernorm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (3): EncoderLayer(\n",
      "        (rga): RelativeGlobalAttention(\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (FFN_pre): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (FFN_suf): Linear(in_features=128, out_features=256, bias=True)\n",
      "        (layernorm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "        (layernorm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (4): EncoderLayer(\n",
      "        (rga): RelativeGlobalAttention(\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (FFN_pre): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (FFN_suf): Linear(in_features=128, out_features=256, bias=True)\n",
      "        (layernorm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "        (layernorm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (5): EncoderLayer(\n",
      "        (rga): RelativeGlobalAttention(\n",
      "          (Wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (Wv): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (FFN_pre): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (FFN_suf): Linear(in_features=128, out_features=256, bias=True)\n",
      "        (layernorm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "        (layernorm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (fc): Linear(in_features=256, out_features=391, bias=True)\n",
      ")\n",
      "| Summary - Device Info : <class 'torch.cuda.device'>\n",
      ">> Train start...\n",
      ">>> [Epoch was updated]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m~\\dev\\python\\DML-Project\\MusicTransformer\\train.py:92\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     90\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     91\u001b[0m mt\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m---> 92\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[43mmt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m metrics \u001b[38;5;241m=\u001b[39m metric_set(sample, batch_y)\n\u001b[0;32m     94\u001b[0m loss \u001b[38;5;241m=\u001b[39m metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\dev\\python\\DML-Project\\MusicTransformer\\model.py:41\u001b[0m, in \u001b[0;36mMusicTransformer.forward\u001b[1;34m(self, x, length, writer)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer:\n\u001b[0;32m     40\u001b[0m     _, _, look_ahead_mask \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mget_masked_with_pad_tensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_seq, x, x, config\u001b[38;5;241m.\u001b[39mpad_token)\n\u001b[1;32m---> 41\u001b[0m     decoder, w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlook_ahead_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m     fc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(decoder)\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fc\u001b[38;5;241m.\u001b[39mcontiguous() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;28;01melse\u001b[39;00m (fc\u001b[38;5;241m.\u001b[39mcontiguous(), [weight\u001b[38;5;241m.\u001b[39mcontiguous() \u001b[38;5;28;01mfor\u001b[39;00m weight \u001b[38;5;129;01min\u001b[39;00m w])\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dml\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\dev\\python\\DML-Project\\MusicTransformer\\custom\\layers.py:232\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[1;34m(self, x, mask)\u001b[0m\n\u001b[0;32m    230\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers):\n\u001b[1;32m--> 232\u001b[0m     x, w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menc_layers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    233\u001b[0m     weights\u001b[38;5;241m.\u001b[39mappend(w)\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x, weights\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dml\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\dev\\python\\DML-Project\\MusicTransformer\\custom\\layers.py:154\u001b[0m, in \u001b[0;36mEncoderLayer.forward\u001b[1;34m(self, x, mask, **kwargs)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 154\u001b[0m     attn_out, w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrga\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    155\u001b[0m     attn_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(attn_out)\n\u001b[0;32m    156\u001b[0m     out1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayernorm1(attn_out\u001b[38;5;241m+\u001b[39mx)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dml\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\dev\\python\\DML-Project\\MusicTransformer\\custom\\layers.py:98\u001b[0m, in \u001b[0;36mRelativeGlobalAttention.forward\u001b[1;34m(self, inputs, mask, **kwargs)\u001b[0m\n\u001b[0;32m     96\u001b[0m QKt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(q, Kt)\n\u001b[0;32m     97\u001b[0m logits \u001b[38;5;241m=\u001b[39m QKt \u001b[38;5;241m+\u001b[39m Srel\n\u001b[1;32m---> 98\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mlogits\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    101\u001b[0m     logits \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (mask\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mint64) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1e9\u001b[39m)\u001b[38;5;241m.\u001b[39mto(logits\u001b[38;5;241m.\u001b[39mdtype)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%run MusicTransformer/train.py -c ./MusicTransformer/config/base.yml ./MusicTransformer/config/train.yml -m ./Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_midi(tensor):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, train_dataset, val_dataset, num_epochs=5):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_midi(model):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_midi(path):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "dataset = load_dataset(dataset_path)\n",
    "train_dataset, val_dataset, test_dataset = train_val_test_split(dataset)\n",
    "trainin_loop(model, train_dataset, val_dataset)\n",
    "midi = generate_midi(model)\n",
    "save_midi(midi)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "08c9fd0368e894f73c4be16302b7b699a9b33ae4acecbe382322bd4c418527fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
